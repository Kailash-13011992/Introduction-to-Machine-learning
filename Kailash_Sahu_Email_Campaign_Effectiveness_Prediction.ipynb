{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "joUafhgw0Drg",
        "ddVg1Z87ayuf",
        "yeT5vCU5a6wz",
        "A-rSrbBVHMXc",
        "7LbfhSrAR9PT",
        "m9A-R7erSL_L",
        "osJDRBP5SQ_W",
        "cPR7xCEQYEDL",
        "sXP9vV4RwMjS",
        "zvkUjLHT_Yyi",
        "DOBKr1nixGHv",
        "asef3OzzzmeS",
        "lTrfqYKIwdxq",
        "7BDtOtPO2bho",
        "Omo_6RJNK78f",
        "RrldNkSXzVgd",
        "lvUQqGfsuBLL",
        "ISaHU1hD-MVV",
        "DYi2Td-G-BiR",
        "5V-FtASy-Qjz",
        "bQB3M4El_Lf5",
        "xyouLThie9fI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kailash-13011992/Introduction-to-Machine-learning/blob/main/Kailash_Sahu_Email_Campaign_Effectiveness_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Title : Email Campaign Effectiveness Prediction**"
      ],
      "metadata": {
        "id": "uMQsVNoQf5sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Kailash-13011992/Introduction-to-Machine-learning"
      ],
      "metadata": {
        "id": "nvAjA85LDYIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Problem Description**\n",
        "\n",
        "Most of the small to medium business owners are making effective use of Gmail-based\n",
        "Email marketing Strategies for offline targeting of converting their prospective customers into\n",
        "leads so that they stay with them in business.\n",
        "The main objective is to create a machine learning model to characterize the mail and track\n",
        "the mail that is ignored; read; acknowledged by the reader.\n",
        "Data columns are self-explanatory."
      ],
      "metadata": {
        "id": "a4mmedc1gH9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Business Context**\n",
        "Email marketing is the act of sending a commercial message, typically to a group of people, using email. In its broadest sense, every email sent to a potential or current customer could be considered email marketing. It involves using email to send advertisements, request business, or solicit sales or donations. Email marketing strategies commonly seek to achieve one or more of three primary objectives, to build loyalty, trust, or brand awareness. The term usually refers to sending email messages with the purpose of enhancing a merchant's relationship with current or previous customers, encouraging customer loyalty and repeat business, acquiring new customers or convincing current customers to purchase something immediately, and sharing third-party ads."
      ],
      "metadata": {
        "id": "QLR3moPliMj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Description**\n",
        "* **Email Id** - It contains the email id's of the customers/individuals\n",
        "* **Email Type** - There are two categories 1 and 2. We can think of them as marketing emails or important updates, notices like emails regarding the business.\n",
        "* **Subject Hotness Score** - It is the email's subject's score on the basis of how good and effective the content is.\n",
        "* **Email Source** - It represents the source of the email like sales and marketing or important admin mails related to the product.\n",
        "* **Email Campaign Type** - The campaign type of the email.\n",
        "* **Total Past Communications** - This column contains the total previous mails from the same source, the number of communications had.\n",
        "* **Customer Location** - Contains demographical data of the customer, the location where the customer resides.\n",
        "* **Time Email sent Category** - It has three categories 1,2 and 3; the time of the day when the email was sent, we can think of it as morning, evening and night time slots.\n",
        "* **Word Count** - The number of words contained in the email.\n",
        "* **Total links** - Number of links in the email.\n",
        "* **Total Images** - Number of images in the email.\n",
        "* **Email Status** - Our target variable which contains whether the mail was ignored, read, acknowledged by the reader."
      ],
      "metadata": {
        "id": "HS6qtIEZm-Lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Collection and Preprocessing**"
      ],
      "metadata": {
        "id": "joUafhgw0Drg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing"
      ],
      "metadata": {
        "id": "ddVg1Z87ayuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDCpxmejfDFu"
      },
      "outputs": [],
      "source": [
        "# Importing important libraries and modules\n",
        "# For data reading and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.rcParams.update({'figure.figsize':(8,5),'figure.dpi':100})\n",
        "\n",
        "# VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Modelling\n",
        "# Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Grid Search for Hyperparameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, roc_auc_score, f1_score, recall_score,roc_curve, classification_report\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the csv dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PROJECTS/Supervised ML - Classification/data_email_campaign.csv')"
      ],
      "metadata": {
        "id": "9S2TnGQhikUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "80b1cf22-0bca-429c-c762-8d603f5b1898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/PROJECTS/Supervised ML - Classification/data_email_campaign.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bcd61b8073eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading the csv dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/PROJECTS/Supervised ML - Classification/data_email_campaign.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/PROJECTS/Supervised ML - Classification/data_email_campaign.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Inspection"
      ],
      "metadata": {
        "id": "yeT5vCU5a6wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the data\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Xgk1vQLht2tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First look of our dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rYE119h5kf4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic info of the data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "6L4WdwrIkimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description of the data\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "1a38ZeSVsDha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().mean()*100"
      ],
      "metadata": {
        "id": "X0jmyd1amqg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above data it cab be observed that 4 features have null values.\\\n",
        "● Customer_Location\\\n",
        "● Total_past_communications\\\n",
        "● Total_Links\\\n",
        "● Total_Images\\\n",
        "We will be handling it in the upcoming Data Cleaning section."
      ],
      "metadata": {
        "id": "Uhz5DS7DVVu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1wAXwjqEuRHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no duplicates in the dataset."
      ],
      "metadata": {
        "id": "1Z626wbdD1tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "A-rSrbBVHMXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Data"
      ],
      "metadata": {
        "id": "7LbfhSrAR9PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#starting with categorical variables\n",
        "categorical_variables = ['Email_Type','Email_Source_Type','Customer_Location','Email_Campaign_Type','Time_Email_sent_Category']\n",
        "Target_variable = ['Email_Status']\n",
        "\n",
        "for i,value in enumerate(categorical_variables):\n",
        "  ax = sns.countplot(x=df[value], hue=df[Target_variable[0]])\n",
        "  unique = len([x for x in df[value].unique() if x==x])\n",
        "  # Bars are created in hue order\n",
        "  bars = ax.patches\n",
        "  for i in range(unique):\n",
        "      catbars=bars[i:][::unique]\n",
        "      #get height\n",
        "      total = sum([x.get_height() for x in catbars])\n",
        "      # Print percentage on the bars\n",
        "      for bar in catbars:\n",
        "        ax.text(bar.get_x()+bar.get_width()/2.,\n",
        "                    bar.get_height(),\n",
        "                    f'{bar.get_height()/total:.0%}',\n",
        "                    ha=\"center\",va=\"bottom\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R6d1ShQTd_21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can observed that the distribution of Email_Status is almost similar in all the categories except in Email_Campaign_Type, it shows a totally different trend. For Email_Campaign_Type = 1 it's only 10% of the customers who are ignoring the email and for 2 around 87% customer ignore the emails."
      ],
      "metadata": {
        "id": "7gbN9bI3I4Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continuous Data"
      ],
      "metadata": {
        "id": "pVhHa2VeSHWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Univariate"
      ],
      "metadata": {
        "id": "m9A-R7erSL_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#continuous variables\n",
        "continuous_variables = ['Subject_Hotness_Score', 'Total_Past_Communications','Word_Count','Total_Links','Total_Images']\n",
        "i = 1\n",
        "fig = plt.figure(figsize = (15,10))\n",
        "for c in list(continuous_variables):\n",
        "    if i <= 3:\n",
        "            ax1 = fig.add_subplot(2,3,i)\n",
        "            sns.boxplot(data = df, x=c, ax = ax1)\n",
        "            ax2 = fig.add_subplot(2,3,i+3)\n",
        "            sns.distplot(df[c], ax=ax2)\n",
        "\n",
        "    i += 1\n",
        "    if i == 4:\n",
        "        fig = plt.figure(figsize = (15,10))\n",
        "        i = 1"
      ],
      "metadata": {
        "id": "gtWEd75kM38H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's evident that **Word Count** and **Total_Past Communications** follow almost a **normal distribution**. The rest of the features were **highly skewed** to the **left**."
      ],
      "metadata": {
        "id": "QJLoj833beyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bivariate"
      ],
      "metadata": {
        "id": "osJDRBP5SQ_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#continuous variables through boxplots\n",
        "fig = plt.figure(figsize = (15,10))\n",
        "i = 1\n",
        "for value in continuous_variables:\n",
        "  if i <= len(continuous_variables):\n",
        "    axes = fig.add_subplot(2,3,i)\n",
        "    ax = sns.boxplot(data = df, x = 'Email_Status', y = value, ax = axes)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "pU5PGcjzcZ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplots, following observations can be made:\n",
        "* For **high Subject_Hotness_Score** the chances of mail getting **ignored** is also **high**.\n",
        "* As the number of **Total_Past_Communication** is **increasing**, the chances of Email getting **ignored is decreasing**.\n",
        "* As the **word_count** increases beyond the **600** mark we see that there is a **high** possibility of that email being **ignored**. The ideal mark is **400–600**."
      ],
      "metadata": {
        "id": "M3esP0PPLJc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Correlation between continuous variables\n",
        "correlation = df[continuous_variables].corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "QHn4eZwoR-H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it can observed that the correlation score is **0.78** for **Total_Images** and **Total_Links** which is on a scale of (-1,1) so it can be understood as a **high positive correlation**."
      ],
      "metadata": {
        "id": "oyEEehT4GCJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning**"
      ],
      "metadata": {
        "id": "cPR7xCEQYEDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data"
      ],
      "metadata": {
        "id": "sXP9vV4RwMjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Customer_Location column from the dataframe\n",
        "df.drop(columns=['Customer_Location'], inplace = True)\n",
        "# Removing Customer_Location from categorical_variables\n",
        "categorical_variables.remove('Customer_Location')"
      ],
      "metadata": {
        "id": "E0kQel6KPGIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's been already seen in our missing values analysis that the **Customer_Location** feature has the **most** number of missing values (16.963411 % missing values). Also, in categorical data analysis, after plotting the frequency graph of different values of Customer_location with respect to the **Email_status** category we found that the percentage ratio of Email being Ignored, Read or Acknowledged is the same **irrespective** of the **Customer_Location**.\\\n",
        "● The Customer_Location feature does not affect Email_Status and it can be dropped"
      ],
      "metadata": {
        "id": "1dTgiq1NPGs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing Total_Past_Communications with the mean\n",
        "df['Total_Past_Communications'].fillna(df['Total_Past_Communications'].mean(),inplace=True)"
      ],
      "metadata": {
        "id": "UE9c0KuEWFkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the continuous data analysis part it's known get that the graph of **Total_past_Communications** follows **approximate Normal Distribution**. So, let's **impute** the missing values by the **mean** of the values."
      ],
      "metadata": {
        "id": "7Chog_OMQMXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing Total_Links with the mode\n",
        "df['Total_Links'].fillna(df['Total_Links'].mode()[0],inplace=True)"
      ],
      "metadata": {
        "id": "snCz5iIOSgRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing Total_Images with the mode\n",
        "df['Total_Images'].fillna(df['Total_Images'].mode()[0],inplace=True)"
      ],
      "metadata": {
        "id": "fM4763AWXSCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the continuous data analysis part it's known that the graph of **Total_Links & Total_Images** is **left skewed**. So, **imputing** the missing values by the **mode** of the values is most appropriate."
      ],
      "metadata": {
        "id": "inYDDs_8QTia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean-up"
      ],
      "metadata": {
        "id": "zvkUjLHT_Yyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping column Email_ID\n",
        "df.drop(columns=['Email_ID'], inplace=True)"
      ],
      "metadata": {
        "id": "xOFSJfkRGGiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it's known it is an ID column so it doesn't add value to our data and it's better to be dropped."
      ],
      "metadata": {
        "id": "8E807BVk_cnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering**"
      ],
      "metadata": {
        "id": "DOBKr1nixGHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multicollinearity"
      ],
      "metadata": {
        "id": "asef3OzzzmeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VIF code\n",
        "def vif_cal(df):\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"variables\"] = df.columns\n",
        "  vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "  return(vif)"
      ],
      "metadata": {
        "id": "emFpbnulwj6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get VIF scores\n",
        "vif_df = vif_cal(df[[i for i in df.describe().columns if i not in categorical_variables + ['Email_Status']]])\n",
        "vif_df"
      ],
      "metadata": {
        "id": "7c0Z2b_cyWuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scatter plot between total images and total links\n",
        "sns.scatterplot(x=df[\"Total_Images\"],y=df[\"Total_Links\"],hue=df['Email_Status'])"
      ],
      "metadata": {
        "id": "lahzpaCiydkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ralation between Total Links and Total Images is almost linear so it would be better to add them together."
      ],
      "metadata": {
        "id": "CqPyWBBkv9jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining total links and total images\n",
        "df['Total_Images_Links'] = df['Total_Images'] + df['Total_Links']\n",
        "# Dropping previous columns\n",
        "df.drop(['Total_Images','Total_Links'],inplace=True,axis=1)"
      ],
      "metadata": {
        "id": "d1BuTwJfysI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check VIF scores\n",
        "vif_df = vif_cal(df[[i for i in df.describe().columns if i not in categorical_variables + ['Email_Status']]])\n",
        "vif_df"
      ],
      "metadata": {
        "id": "WWRamicMzwmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outliers Treatment"
      ],
      "metadata": {
        "id": "lTrfqYKIwdxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing dropped columns from the dataset\n",
        "continuous_variables.remove('Total_Images')\n",
        "continuous_variables.remove('Total_Links')\n",
        "# Adding the combined column\n",
        "continuous_variables.append('Total_Images_Links')"
      ],
      "metadata": {
        "id": "N1H0SQVmz19K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "X5coKKpmWtm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the outliers in continuous variables\n",
        "sns.boxplot(data = df[continuous_variables], orient='h', dodge=False)"
      ],
      "metadata": {
        "id": "BZ0RK-TM0MpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature **Word_Count** has **no** outliers."
      ],
      "metadata": {
        "id": "MPsT62S3XJKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Word_Count column as it has no outliers\n",
        "continuous_variables.remove('Word_Count')\n",
        "# Creating an empty dictionary to store the count of each Email_Status\n",
        "outliers = {}\n",
        "for elem in continuous_variables:\n",
        "  # Finding Quartile\n",
        "  q_75, q_25 = np.percentile(df.loc[:,elem],[75,25])\n",
        "  # Calculating Inter Quartile Range\n",
        "  IQR = q_75-q_25\n",
        "  # Fixing Boundaries for outliers\n",
        "  max = q_75+(1.5*IQR)\n",
        "  min = q_25-(1.5*IQR)\n",
        "  # An empty list to store email_status of only outliers\n",
        "  outlier_list=[]\n",
        "  outlier_list=df.loc[df[elem] < min]['Email_Status'].tolist()\n",
        "  outlier_list.append(df.loc[df[elem] > max]['Email_Status'].tolist())\n",
        "  outliers[elem]={}\n",
        "  for i in outlier_list[0]:\n",
        "      outliers[elem][i] = outliers[elem].get(i,0) + 1\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "Lv7DaFNvz4PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dependent variable is highly imbalanced so before dropping outliers it must be checked that it will not delete more than 5% of the minority class which is Email_Status =1,2."
      ],
      "metadata": {
        "id": "0bkrz-FtZOOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the percentage of minority classs going to be affected by outliers\n",
        "sum_min=0\n",
        "sum_maj=0\n",
        "for x in [y for y in continuous_variables]:\n",
        "  sum_min += outliers[x][1]\n",
        "  sum_min += outliers[x][2]\n",
        "  sum_maj += outliers[x][0]\n",
        "total=df.groupby('Email_Status').count()['Email_Type'][1]+df.groupby('Email_Status').count()['Email_Type'][2]\n",
        "total_0=df.groupby('Email_Status').count()['Email_Type'][0]\n",
        "print(\"Percentage of majority class having outliers = \",100*sum_maj/total_0)\n",
        "print(\"Percentage of minority class having outliers = \",100*sum_min/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iks35TYNZB8H",
        "outputId": "706ab0c7-679b-4446-afd7-23ce39160bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'continuous_variables' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f552c5feb624>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msum_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msum_maj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontinuous_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msum_min\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0msum_min\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'continuous_variables' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be understood that close to 5% of data was being removed from minority class. Hence decided against removing the outliers. This problem can be solved through normalization and choosing boosted trees for our modelling which are robust to outliers."
      ],
      "metadata": {
        "id": "Qpv7tJnLbDOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting majority outliers\n",
        "for elem in continuous_variables:\n",
        "  q_low = df[elem].quantile(0.01)\n",
        "  q_high  = df[elem].quantile(0.99)\n",
        "  df = df.drop(df[(df[elem] > q_high) &  (df['Email_Status']==0)].index)\n",
        "  df = df.drop(df[(df[elem] < q_low) & (df['Email_Status']==0)].index)"
      ],
      "metadata": {
        "id": "EZ_lVe6EbNnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variables"
      ],
      "metadata": {
        "id": "76EaiRRLD3sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variables\n",
        "df = pd.get_dummies(df,columns=categorical_variables, drop_first=True)\n",
        "# as some features had binary categories, we are going to delete one of them to keep it binary encoded and have less columns\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "mbLBmTAQFwJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "6_sNMfPbIMoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling"
      ],
      "metadata": {
        "id": "7BDtOtPO2bho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add word count back to the continuous variabl\n",
        "continuous_variables.append('Word_Count')"
      ],
      "metadata": {
        "id": "mTz3flVVl2zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit scaler to the train set, it will learn the parameters\n",
        "scaler.fit(df[continuous_variables])\n",
        "\n",
        "# Transform train and test sets\n",
        "df[continuous_variables] = scaler.transform(df[continuous_variables])"
      ],
      "metadata": {
        "id": "gwFSA81v2grb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Email_Status', axis = 1), df['Email_Status'], test_size=0.20, random_state = 42, stratify = df['Email_Status'])"
      ],
      "metadata": {
        "id": "6x1qp18iIQvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to stratify to get same proprtion of classes in both the sets."
      ],
      "metadata": {
        "id": "733vB0PDKiI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "dSXcp9qbJZHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Imbalance"
      ],
      "metadata": {
        "id": "Omo_6RJNK78f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing our imbalanced dataset\n",
        "ax = sns.countplot(x=df['Email_Status'])\n",
        "totals = []\n",
        "for i in ax.patches:\n",
        "    totals.append(i.get_height())\n",
        "\n",
        "total = sum(totals)\n",
        "\n",
        "for i in ax.patches:\n",
        "    ax.text(i.get_x() - .01, i.get_height() + .5, \\\n",
        "          str(round((i.get_height()/total)*100, 2))+'%', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gY7qVQD2LDRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only around 3.5% of observations are classified as acknowledged emails and 80% are ignored emails. This will create a bias in favour of ignored emails in the model."
      ],
      "metadata": {
        "id": "BFJesN60Lhz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Undersampling"
      ],
      "metadata": {
        "id": "RrldNkSXzVgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
        "x_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print('Original dataset shape:', len(df))\n",
        "print('Resampled dataset shape', len(y_train_rus))"
      ],
      "metadata": {
        "id": "QJU26qvrP-D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(Counter(df['Email_Status']).keys(), Counter(df['Email_Status']).values())\n",
        "plt.title(\"Before Undersampling\")"
      ],
      "metadata": {
        "id": "UgJPY5Kes7CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(Counter(y_train_rus).keys(), Counter(y_train_rus).values())\n",
        "plt.title(\"After Undersampling\")"
      ],
      "metadata": {
        "id": "t7HWPh0Ys9Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_elements, count_of_elements = np.unique(y_train_rus, return_counts=True)\n",
        "print(\"Frequency of the unique values of Email_Status:\")\n",
        "print(np.asarray((unique_elements, count_of_elements)))"
      ],
      "metadata": {
        "id": "u0uB8K_wtC7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Under Sampler created a balanced dataset of 2373 records."
      ],
      "metadata": {
        "id": "nI7jCl7RtoCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SMOTE (Synthetic Minority Oversampling Technique)"
      ],
      "metadata": {
        "id": "lvUQqGfsuBLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "\n",
        "# Fit predictor and target variable\n",
        "x_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "print('Original dataset shape', len(y_train))\n",
        "print('Resampled dataset shape', len(y_train_smote))"
      ],
      "metadata": {
        "id": "DqysQ0X7tK0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(Counter(y_train_smote).keys(), Counter(y_train_smote).values())\n",
        "plt.title(\"After Undersampling\")"
      ],
      "metadata": {
        "id": "nyVZZ18zuL_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Implementation and Evaluation**"
      ],
      "metadata": {
        "id": "06NQvleRu7Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Columns needed to compare metrics\n",
        "comparison_columns = ['Model_Name', 'Train_Accuracy', 'Train_Recall', 'Train_Precision', 'Train_F1score', 'Train_AUC' ,'Test_Accuracy', 'Test_Recall', 'Test_Precision', 'Test_F1score', 'Test_AUC']"
      ],
      "metadata": {
        "id": "zlHHV4Bn4gI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_name_RUS,model_name_SMOTE,model_var_rus, model_var_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test):\n",
        "  ''' This function predicts, evaluates various models for clasification using Random Undersampling and SMOTE algorithms, visualizes results\n",
        "      and creates a dataframe that compares the various models.'''\n",
        "\n",
        "  #Making predictions random undersampling\n",
        "  y_pred_rus_train = model_var_rus.predict(x_train_rus)\n",
        "  y_pred_rus_test = model_var_rus.predict(X_test)\n",
        "  #probs\n",
        "  train_rus_proba = model_var_rus.predict_proba(x_train_rus)\n",
        "  test_rus_proba = model_var_rus.predict_proba(X_test)\n",
        "\n",
        "  #Making predictions smote\n",
        "  y_pred_smote_train = model_var_smote.predict(x_train_smote)\n",
        "  y_pred_smote_test = model_var_smote.predict(X_test)\n",
        "  #probs\n",
        "  train_sm_proba = model_var_smote.predict_proba(x_train_smote)\n",
        "  test_sm_proba = model_var_smote.predict_proba(X_test)\n",
        "\n",
        "  #Evaluation\n",
        "  #Accuracy RUS\n",
        "  accuracy_rus_train = accuracy_score(y_train_rus,y_pred_rus_train)\n",
        "  accuracy_rus_test = accuracy_score(y_test,y_pred_rus_test)\n",
        "  #Accuracy SMOTE\n",
        "  accuracy_smote_train = accuracy_score(y_train_smote,y_pred_smote_train)\n",
        "  accuracy_smote_test = accuracy_score(y_test,y_pred_smote_test)\n",
        "\n",
        "  #Confusion Matrix RUS\n",
        "  cm_rus_train = confusion_matrix(y_train_rus,y_pred_rus_train)\n",
        "  cm_rus_test = confusion_matrix(y_test,y_pred_rus_test)\n",
        "  #Confusion Matrix SMOTE\n",
        "  cm_smote_train = confusion_matrix(y_train_smote,y_pred_smote_train)\n",
        "  cm_smote_test = confusion_matrix(y_test,y_pred_smote_test)\n",
        "\n",
        "  #Recall RUS\n",
        "  train_recall_rus = recall_score(y_train_rus,y_pred_rus_train, average='weighted')\n",
        "  test_recall_rus = recall_score(y_test,y_pred_rus_test, average='weighted')\n",
        "  #Recall SMOTE\n",
        "  train_recall_smote = recall_score(y_train_smote,y_pred_smote_train, average='weighted')\n",
        "  test_recall_smote = recall_score(y_test,y_pred_smote_test, average='weighted')\n",
        "\n",
        "  #Precision RUS\n",
        "  train_precision_rus = precision_score(y_train_rus,y_pred_rus_train, average='weighted')\n",
        "  test_precision_rus = precision_score(y_test,y_pred_rus_test, average='weighted')\n",
        "  #Precision SMOTE\n",
        "  train_precision_smote = precision_score(y_train_smote,y_pred_smote_train, average='weighted')\n",
        "  test_precision_smote = precision_score(y_test,y_pred_smote_test, average='weighted')\n",
        "\n",
        "  #F1 Score RUS\n",
        "  train_f1_rus = f1_score(y_train_rus,y_pred_rus_train, average='weighted')\n",
        "  test_f1_rus = f1_score(y_test,y_pred_rus_test, average='weighted')\n",
        "  #F1 Score SMOTE\n",
        "  train_f1_smote = f1_score(y_train_smote,y_pred_smote_train, average='weighted')\n",
        "  test_f1_smote = f1_score(y_test,y_pred_smote_test, average='weighted')\n",
        "\n",
        "  #ROC-AUC RUS\n",
        "  train_auc_rus = roc_auc_score(y_train_rus,train_rus_proba,average='weighted',multi_class = 'ovr')\n",
        "  test_auc_rus = roc_auc_score(y_test,test_rus_proba,average='weighted',multi_class = 'ovr')\n",
        "  #ROC-AUC SMOTE\n",
        "  train_auc_smote = roc_auc_score(y_train_smote,train_sm_proba,average='weighted',multi_class = 'ovr')\n",
        "  test_auc_smote = roc_auc_score(y_test,test_sm_proba,average='weighted',multi_class = 'ovr')\n",
        "\n",
        "  #Visualising Results RUS\n",
        "  print(\"----- Evaluation on Random Undersampled data -----\" + str(model_name_RUS) + \"------\")\n",
        "  print(\"--------------Test data ---------------\\n\")\n",
        "  print(\"Confusion matrix \\n\")\n",
        "  print(cm_rus_test)\n",
        "  print(classification_report(y_test,y_pred_rus_test))\n",
        "\n",
        "  #create ROC curve\n",
        "  fpr = {}\n",
        "  tpr = {}\n",
        "  thresh ={}\n",
        "  no_of_class=3\n",
        "  for i in range(no_of_class):\n",
        "      fpr[i], tpr[i], thresh[i] = metrics.roc_curve(y_test, test_rus_proba[:,i], pos_label=i)\n",
        "  plt.plot(fpr[0], tpr[0], linestyle='--',color='blue', label='Class 0 vs Others'+\"AUC=\"+str(test_auc_rus))\n",
        "  plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Others'+\"AUC=\"+str(test_auc_rus))\n",
        "  plt.plot(fpr[2], tpr[2], linestyle='--',color='orange', label='Class 2 vs Others'+\"AUC=\"+str(test_auc_rus))\n",
        "  plt.title('Multiclass ROC curve of ' + str(model_name_RUS))\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  #Visualising Results SMOTE\n",
        "  print(\"----- Evaluation on SMOTE data -------\" + str(model_name_SMOTE) + '-----')\n",
        "  print(\"---------------Test data ---------------\\n\")\n",
        "  print(\"Confusion matrix \\n\")\n",
        "  print(cm_smote_test)\n",
        "  print(classification_report(y_test,y_pred_smote_test))\n",
        "\n",
        "  #create ROC curve\n",
        "  fpr = {}\n",
        "  tpr = {}\n",
        "  thresh ={}\n",
        "  no_of_class=3\n",
        "  for i in range(no_of_class):\n",
        "      fpr[i], tpr[i], thresh[i] = metrics.roc_curve(y_test, test_sm_proba[:,i], pos_label=i)\n",
        "  plt.plot(fpr[0], tpr[0], linestyle='--',color='blue', label='Class 0 vs Others'+\" AUC=\"+str(test_auc_smote))\n",
        "  plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Others'+\" AUC=\"+str(test_auc_smote))\n",
        "  plt.plot(fpr[2], tpr[2], linestyle='--',color='orange', label='Class 2 vs Others'+\" AUC=\"+str(test_auc_smote))\n",
        "  plt.title('Multiclass ROC curve of '+ str(model_name_SMOTE))\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  #Saving our results\n",
        "  global comparison_columns\n",
        "  metric_scores_rus = [model_name_RUS,accuracy_rus_train,train_recall_rus,train_precision_rus,train_f1_rus,train_auc_rus,accuracy_rus_test,test_recall_rus,test_precision_rus,test_f1_rus,test_auc_rus]\n",
        "  final_dict_rus = dict(zip(comparison_columns,metric_scores_rus))\n",
        "\n",
        "  metric_scores_smote = [model_name_SMOTE,accuracy_smote_train,train_recall_smote,train_precision_smote,train_f1_smote,train_auc_smote,accuracy_smote_test,test_recall_smote,test_precision_smote,test_f1_smote,test_auc_smote]\n",
        "  final_dict_smote = dict(zip(comparison_columns,metric_scores_smote))\n",
        "\n",
        "  dict_list = [final_dict_rus, final_dict_smote]\n",
        "  return dict_list"
      ],
      "metadata": {
        "id": "CtJtDmTQutNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create the comparison table\n",
        "final_list = []\n",
        "def add_list_to_final_df(dict_list):\n",
        "  global final_list\n",
        "  for elem in dict_list:\n",
        "    final_list.append(elem)\n",
        "  global comparison_df\n",
        "  comparison_df = pd.DataFrame(final_list, columns= comparison_columns)"
      ],
      "metadata": {
        "id": "yuuK88114V4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "Ke9sgomZvBfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Fitting Random Under Sampling\n",
        "logistic_rus = LogisticRegression(class_weight='balanced',multi_class='multinomial', solver='lbfgs')\n",
        "logistic_rus.fit(x_train_rus, y_train_rus)"
      ],
      "metadata": {
        "id": "XHjvJq0d3_rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting on smote\n",
        "logistic_smote = LogisticRegression(class_weight='balanced',multi_class='multinomial', solver='lbfgs')\n",
        "logistic_smote.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "1CfxTgh74Udm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's evaluate logistic regression\n",
        "logistic_reg_list = model_evaluation('Logistic Regression RUS','Logistic Regression SMOTE',logistic_rus, logistic_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "logistic_reg_list"
      ],
      "metadata": {
        "id": "ytoGQPDU4tA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding results to final list\n",
        "add_list_to_final_df(logistic_reg_list)"
      ],
      "metadata": {
        "id": "UpOIFF1G40IS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "1cb741b7-790d-4ae3-946a-a06f06d21eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'add_list_to_final_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b285973f6823>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Adding results to final list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madd_list_to_final_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_reg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'add_list_to_final_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "65lYwoMx49rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "ISaHU1hD-MVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "QG_kgkch5FKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDaipRssnfRS"
      },
      "outputs": [],
      "source": [
        "# Applying Classifier using Random under sampling\n",
        "dt_rus = DecisionTreeClassifier()\n",
        "dt_rus.fit(x_train_rus,y_train_rus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Classifier using SMOTE\n",
        "dt_smote = DecisionTreeClassifier()\n",
        "dt_smote.fit(x_train_smote,y_train_smote)"
      ],
      "metadata": {
        "id": "O3aLwura5cG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating Dcision Tree Classifier\n",
        "dt_eval_list = model_evaluation('Decision Tree RUS', 'Decision Tree SMOTE', dt_rus, dt_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "dt_eval_list"
      ],
      "metadata": {
        "id": "ML4ydRap57IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(dt_eval_list)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "UFpbgo9E57vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "DYi2Td-G-BiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_rus = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "\n",
        "# Fit the model on the train set\n",
        "knn_rus.fit(x_train_rus,y_train_rus)"
      ],
      "metadata": {
        "id": "zjke9MN_SZX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Classifier SMOTE\n",
        "knn_smote = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "\n",
        "# Fit the model on the train set\n",
        "knn_smote.fit(x_train_smote,y_train_smote)"
      ],
      "metadata": {
        "id": "t0brU4NrTRo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Evaluation\n",
        "knn_eval_list = model_evaluation('KNN RUS', 'KNN SMOTE', knn_rus, knn_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "knn_eval_list"
      ],
      "metadata": {
        "id": "zmuktp2NTqdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(knn_eval_list)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "lqn8v5Qo8a6F",
        "outputId": "16a25f93-1e1a-458f-e77d-9b3e904fd1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'add_list_to_final_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b7b43223c4ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Updating the results list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madd_list_to_final_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_eval_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Having a look at our final comparison dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomparison_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'add_list_to_final_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "5V-FtASy-Qjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Applying Classifier using Random under sampling\n",
        "rf_rus = RandomForestClassifier(random_state=42, max_depth=5, n_estimators=100, oob_score=True)\n",
        "rf_rus.fit(x_train_rus,y_train_rus)"
      ],
      "metadata": {
        "id": "8Mxptprk6lv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Classifier SMOTE\n",
        "rf_smote = RandomForestClassifier(random_state=42, max_depth=5, n_estimators=100, oob_score=True)\n",
        "rf_smote.fit(x_train_smote,y_train_smote)"
      ],
      "metadata": {
        "id": "DFQuuPMd7GB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Evaluation\n",
        "rf_eval_list = model_evaluation('Random Forest RUS', 'Random Forest SMOTE', rf_rus, rf_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "rf_eval_list"
      ],
      "metadata": {
        "id": "QWksu0VR-WBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(rf_eval_list)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "XTGvfElJ-iPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "bQB3M4El_Lf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the classifier\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Parameter dictionary\n",
        "params = {'max_depth': [3,5,10,20],\n",
        "          'min_samples_leaf': [5,10,20,50,100],\n",
        "          'n_estimators': [10,25,30,50,100,200]}\n",
        "\n",
        "# Grid Search to get the best parameters\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=params, cv = 4, n_jobs=-1, verbose=1, scoring=\"f1_weighted\")\n",
        "\n",
        "# Fitting Random Under Sampling to grid search\n",
        "grid_search.fit(x_train_rus,y_train_rus)"
      ],
      "metadata": {
        "id": "QvgwoPSe-mTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters\n",
        "rf_tuned_rus = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "neecGXsQ_YmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting SMOTE to grid search\n",
        "grid_search_smote = GridSearchCV(estimator=rf, param_grid=params, cv = 4, n_jobs=-1, verbose=1, scoring=\"f1_weighted\")\n",
        "grid_search_smote.fit(x_train_smote,y_train_smote)\n",
        "\n",
        "# Best smote Parameters\n",
        "rf_tuned_smote = grid_search_smote.best_estimator_"
      ],
      "metadata": {
        "id": "-zSlJFGSAJ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation for Random Forest Hyperparameter Tuned model\n",
        "rf_tuned_list = model_evaluation('Random Forest Tuned RUS', 'Random Forest Tuned SMOTE', rf_tuned_rus, rf_tuned_smote,x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "rf_tuned_list"
      ],
      "metadata": {
        "id": "xudcc9XvAhVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(rf_tuned_list)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "aUzLdbllArpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "eb2bde16-e1bf-4474-dc52-fd28f46ba833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'add_list_to_final_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8dfec111edcd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Updating the results list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madd_list_to_final_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_tuned_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Having a look at our final comparison dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomparison_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'add_list_to_final_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance given by hyperparameter random forest tuned model\n",
        "feature_imp = pd.DataFrame({\"Variable\": x_train_smote.columns,\"Importance\": rf_tuned_smote.feature_importances_})\n",
        "feature_imp.sort_values(by=\"Importance\", ascending=False, inplace = True)"
      ],
      "metadata": {
        "id": "7pyHHrCNA1HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing feature importance\n",
        "sns.barplot(x=feature_imp['Importance'],y= feature_imp['Variable'])"
      ],
      "metadata": {
        "id": "vvkSYhv7dgNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping irrelevant features\n",
        "x_train_smote1 = x_train_smote.drop(['Time_Email_sent_Category_3','Time_Email_sent_Category_2','Email_Campaign_Type_3'],axis=1)\n",
        "x_train_rus1 = x_train_rus.drop(['Time_Email_sent_Category_3','Time_Email_sent_Category_2','Email_Campaign_Type_3'],axis=1)\n",
        "X_test1 = X_test.drop(['Time_Email_sent_Category_3','Time_Email_sent_Category_2','Email_Campaign_Type_3'],axis=1)"
      ],
      "metadata": {
        "id": "4B7vGbKXcP3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search to get the best parameters for RUS\n",
        "grid_search_rus = GridSearchCV(estimator=rf, param_grid=params, cv = 4, n_jobs=-1, verbose=1, scoring=\"f1_weighted\")\n",
        "# Fitting RUS to grid search\n",
        "grid_search_rus.fit(x_train_rus1,y_train_rus)\n",
        "# Optimal model\n",
        "rf_tuned_rus1 = grid_search_rus.best_estimator_"
      ],
      "metadata": {
        "id": "zHNJeOe1cVED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting SMOTE\n",
        "grid_search_smote1 = GridSearchCV(estimator=rf, param_grid=params, cv = 4, n_jobs=-1, verbose=1, scoring=\"f1_weighted\")\n",
        "grid_search_smote1.fit(x_train_smote1,y_train_smote)\n",
        "# Optimal smote model\n",
        "rf_tuned_smote1 = grid_search_smote1.best_estimator_"
      ],
      "metadata": {
        "id": "xYMfnYUmcmMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation for Hyperparameter tuned  Random Forest with feature selection\n",
        "rf_tuned_list1 = model_evaluation('Random Forest Tuned RUS FSelect', 'Random Forest Tuned SMOTE FSelect', rf_tuned_rus1, rf_tuned_smote1,x_train_rus1, y_train_rus, x_train_smote1, y_train_smote, X_test1, y_test)\n",
        "rf_tuned_list1"
      ],
      "metadata": {
        "id": "8Sb8U340cpLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(rf_tuned_list1)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "WWBSz8n2e4r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "xyouLThie9fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Fitting rus\n",
        "xgb_rus = XGBClassifier(n_estimators=100, max_depth=12, min_samples_leaf=20, min_samples_split=30)\n",
        "xgb_rus.fit(x_train_rus, y_train_rus)"
      ],
      "metadata": {
        "id": "y3u2QaxNe6ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting smote\n",
        "xgb_smote = XGBClassifier(n_estimators=100, max_depth=12, min_samples_leaf=20, min_samples_split=30)\n",
        "xgb_smote.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "7k11qJ6jfIXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation of XGB\n",
        "xgb_eval_list = model_evaluation('XGBoost RUS', 'XGBoost SMOTE', xgb_rus, xgb_smote, x_train_rus, y_train_rus, x_train_smote, y_train_smote, X_test, y_test)\n",
        "xgb_eval_list"
      ],
      "metadata": {
        "id": "c1nSCEqVfMwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising feature importance of XGBoost Classifier\n",
        "feature_imp_xgb = pd.DataFrame({\"Variable\": x_train_smote.columns,\"Importance\": xgb_smote.feature_importances_})\n",
        "feature_imp_xgb.sort_values(by=\"Importance\", ascending=False, inplace = True)\n",
        "sns.barplot(x=feature_imp_xgb['Importance'], y= feature_imp_xgb['Variable'])"
      ],
      "metadata": {
        "id": "FxZ2x8J1fvzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the results list\n",
        "add_list_to_final_df(xgb_eval_list)\n",
        "# Having a look at our final comparison dataframe\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "U8ZOKMm0fT0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of all the Models"
      ],
      "metadata": {
        "id": "Zf6NM00cU2Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing comparison of f1 score for all models\n",
        "# Creating subplots\n",
        "ax = plt.subplots()\n",
        "\n",
        "ax = sns.pointplot(y=comparison_df['Model_Name'], x = comparison_df['Test_F1score'], color='g', labels=('Test_F1score'))\n",
        "ax = sns.pointplot(y=comparison_df['Model_Name'], x = comparison_df['Train_F1score'], color='r', labels=('Train_F1score'))\n",
        "\n",
        "# Renaming the axes\n",
        "ax.set(xlabel=\"Score\", ylabel=\"Model_Name\")\n",
        "ax.legend(handles=ax.lines[::len(comparison_df)+1], labels=[\"Test_F1score\",\"Train_F1score\"])\n",
        "\n",
        "ax.set_xticklabels([t.get_text().split(\"T\")[0] for t in ax.get_xticklabels()])\n",
        "# Visulaizing illustration\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sLMwv2NkfzKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing comparison of auc score for all models\n",
        "# Creating subplots\n",
        "ax = plt.subplots()\n",
        "\n",
        "ax = sns.pointplot(y=comparison_df['Model_Name'], x = comparison_df['Test_AUC'], color='g', labels=('Test_AUC'))\n",
        "ax = sns.pointplot(y=comparison_df['Model_Name'], x = comparison_df['Train_AUC'], color='r', labels=('Train_AUC'))\n",
        "\n",
        "# Renaming the axes\n",
        "ax.set(xlabel=\"Score\", ylabel=\"Model_Name\")\n",
        "ax.legend(handles=ax.lines[::len(comparison_df)+1], labels=[\"Test_AUC\",\"Train_AUC\"])\n",
        "\n",
        "ax.set_xticklabels([t.get_text().split(\"T\")[0] for t in ax.get_xticklabels()])\n",
        "# Visulaizing illustration\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HpKyAn1-iLdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions**"
      ],
      "metadata": {
        "id": "wnv8ew1MvxCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It could be observed from the EDA that **Email_Campaign_Type** was the **most important** feature. If the Email_Campaign_Type was **1**, there is a **90%** likelihood of your Email to be **acknowledged**.\n",
        "\n",
        "* It was observed that both **Time_Email_Sent and Customer_Location** were insignificant in determining the **Email_status**. The ratio of the Email_Status was same **irrespective** of the time frame the emails were sent on.\n",
        "\n",
        "* As the **word_count** increases beyond the **600** mark we see that there is a **high** possibility of that email being **ignored**. The ideal mark was **400-600**.\n",
        "\n",
        "* For modelling, it was observed that for **imbalance handling** Oversampling i.e. **SMOTE** worked way better than **undersampling** as the latter resulted in a lot of loss of information.\n",
        "\n",
        "* **Decision Tree Model** was **overfitting** as it was working really good on train data but bad on test data.\n",
        "\n",
        "* **Hyperparameter tuning** wasn't able to improve the results to a better extent and casused a lot computaional time.\n",
        "\n",
        "* **XGBoost Algorithm** worked in the **best** way possible with such an imbalanced data having outliers, followed by Random Forest Hyperparameter Tuned model after feature selection with F1 Score of 0.75 on the test set."
      ],
      "metadata": {
        "id": "s1p71EEDv2aY"
      }
    }
  ]
}